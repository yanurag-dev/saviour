package agent

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"time"

	"github.com/anurag/saviour/internal/collector"
	"github.com/anurag/saviour/internal/config"
	"github.com/anurag/saviour/internal/docker"
	"github.com/anurag/saviour/pkg/metrics"
)

// Agent represents the monitoring agent
type Agent struct {
	config          *config.Config
	systemCollector *collector.SystemCollector
	dockerCollector *collector.DockerCollector
	logger          *log.Logger
}

// New creates a new agent instance
func New(cfg *config.Config, logger *log.Logger) (*Agent, error) {
	agent := &Agent{
		config:          cfg,
		systemCollector: collector.NewSystemCollector(cfg.Agent.Name, cfg.Metrics.DiskMounts),
		logger:          logger,
	}

	// Initialize Docker collector if enabled
	if cfg.Metrics.Docker.Enabled {
		filterConfig := docker.FilterConfig{
			MonitorAll: cfg.Metrics.Docker.MonitorAll,
			Labels:     cfg.Metrics.Docker.Filters.Labels,
			Names:      cfg.Metrics.Docker.Filters.Names,
			Images:     cfg.Metrics.Docker.Filters.Images,
		}

		dockerCollector, err := collector.NewDockerCollector(
			cfg.Metrics.Docker.Socket,
			filterConfig,
			logger,
		)
		if err != nil {
			return nil, fmt.Errorf("failed to initialize Docker collector: %w", err)
		}
		agent.dockerCollector = dockerCollector
		logger.Println("âœ“ Docker monitoring enabled")
	}

	return agent, nil
}

// Run starts the agent's main loop
func (a *Agent) Run(ctx context.Context) error {
	a.logger.Printf("Agent '%s' starting...", a.config.Agent.Name)
	a.logger.Printf("Collection interval: %v", a.config.Agent.CollectInterval)

	ticker := time.NewTicker(a.config.Agent.CollectInterval)
	defer ticker.Stop()

	// Collect immediately on start
	if err := a.collectAndProcess(); err != nil {
		a.logger.Printf("Error during initial collection: %v", err)
	}

	// Main loop
	for {
		select {
		case <-ctx.Done():
			a.logger.Println("Agent shutting down...")
			return ctx.Err()
		case <-ticker.C:
			if err := a.collectAndProcess(); err != nil {
				a.logger.Printf("Error collecting metrics: %v", err)
			}
		}
	}
}

func (a *Agent) collectAndProcess() error {
	ctx := context.Background()
	
	// Collect system metrics
	m, err := a.systemCollector.Collect()
	if err != nil {
		return fmt.Errorf("collection failed: %w", err)
	}

	// Collect Docker container metrics if enabled
	if a.dockerCollector != nil {
		containers, err := a.dockerCollector.Collect(ctx)
		if err != nil {
			a.logger.Printf("Warning: Docker collection failed: %v", err)
		} else {
			// Convert docker.ContainerInfo to metrics.ContainerMetrics
			m.Containers = make([]metrics.ContainerMetrics, len(containers))
			for i, c := range containers {
				m.Containers[i] = metrics.ContainerMetrics{
					ID:              c.ID,
					Name:            c.Name,
					Image:           c.Image,
					ImageID:         c.ImageID,
					Labels:          c.Labels,
					State:           c.State,
					Status:          c.Status,
					Health:          c.Health,
					ExitCode:        c.ExitCode,
					OOMKilled:       c.OOMKilled,
					RestartCount:    c.RestartCount,
					Created:         c.Created,
					StartedAt:       c.StartedAt,
					FinishedAt:      c.FinishedAt,
					CPUPercent:      c.CPUPercent,
					MemoryUsage:     c.MemoryUsage,
					MemoryLimit:     c.MemoryLimit,
					MemoryPercent:   c.MemoryPercent,
					NetworkRxBytes:  c.NetworkRxBytes,
					NetworkTxBytes:  c.NetworkTxBytes,
					BlockReadBytes:  c.BlockReadBytes,
					BlockWriteBytes: c.BlockWriteBytes,
					PIDs:            c.PIDs,
				}
			}
		}
	}

	// For now, just log the metrics (later we'll send to server)
	if err := a.processMetrics(m); err != nil {
		return fmt.Errorf("processing failed: %w", err)
	}

	return nil
}

func (a *Agent) processMetrics(m *metrics.SystemMetrics) error {
	// Check alert thresholds
	a.checkAlerts(m)

	// Output metrics (for now, just pretty print)
	// In production, this would send to the central server
	a.logMetrics(m)

	return nil
}

func (a *Agent) checkAlerts(m *metrics.SystemMetrics) {
	// System alerts
	if m.CPU.UsagePercent > a.config.Alerts.CPUThreshold {
		a.logger.Printf("âš ï¸  ALERT: CPU usage (%.2f%%) exceeds threshold (%.2f%%)",
			m.CPU.UsagePercent, a.config.Alerts.CPUThreshold)
	}

	if m.Memory.UsedPercent > a.config.Alerts.MemoryThreshold {
		a.logger.Printf("âš ï¸  ALERT: Memory usage (%.2f%%) exceeds threshold (%.2f%%)",
			m.Memory.UsedPercent, a.config.Alerts.MemoryThreshold)
	}

	for _, disk := range m.Disk {
		if disk.UsedPercent > a.config.Alerts.DiskThreshold {
			a.logger.Printf("âš ï¸  ALERT: Disk usage on %s (%.2f%%) exceeds threshold (%.2f%%)",
				disk.MountPoint, disk.UsedPercent, a.config.Alerts.DiskThreshold)
		}
	}

	// Container alerts
	if a.dockerCollector != nil {
		a.checkContainerAlerts(m.Containers)
	}
}

func (a *Agent) checkContainerAlerts(containers []metrics.ContainerMetrics) {
	defaultThreshold := a.config.Metrics.Docker.Alerts.Default

	for _, container := range containers {
		// Get threshold for this container (check overrides)
		cpuThreshold := defaultThreshold.CPUThreshold
		memThreshold := defaultThreshold.MemoryThreshold
		restartThreshold := defaultThreshold.RestartThreshold

		for _, override := range a.config.Metrics.Docker.Alerts.Overrides {
			if container.Name == override.Name {
				if override.CPUThreshold > 0 {
					cpuThreshold = override.CPUThreshold
				}
				if override.MemoryThreshold > 0 {
					memThreshold = override.MemoryThreshold
				}
				if override.RestartThreshold > 0 {
					restartThreshold = override.RestartThreshold
				}
				break
			}
		}

		// Container state alerts
		if container.State == "exited" {
			a.logger.Printf("ğŸ’€ ALERT: Container '%s' stopped (exit code: %d)",
				container.Name, container.ExitCode)
		}

		if container.Health == "unhealthy" {
			a.logger.Printf("ğŸ¥ ALERT: Container '%s' is unhealthy",
				container.Name)
		}

		if container.OOMKilled {
			a.logger.Printf("ğŸ’¥ ALERT: Container '%s' was OOM killed",
				container.Name)
		}

		// Resource alerts (only for running containers)
		if container.State == "running" {
			if container.CPUPercent > cpuThreshold {
				a.logger.Printf("âš ï¸  ALERT: Container '%s' CPU (%.2f%%) exceeds threshold (%.2f%%)",
					container.Name, container.CPUPercent, cpuThreshold)
			}

			if container.MemoryPercent > memThreshold {
				a.logger.Printf("âš ï¸  ALERT: Container '%s' memory (%.2f%%) exceeds threshold (%.2f%%)",
					container.Name, container.MemoryPercent, memThreshold)
			}
		}

		// Restart count alert
		if container.RestartCount > restartThreshold {
			a.logger.Printf("ğŸ”„ ALERT: Container '%s' restart count (%d) exceeds threshold (%d)",
				container.Name, container.RestartCount, restartThreshold)
		}
	}
}

func (a *Agent) logMetrics(m *metrics.SystemMetrics) {
	a.logger.Println("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
	a.logger.Printf("ğŸ“Š Metrics collected at %s", m.Timestamp.Format(time.RFC3339))
	a.logger.Println("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

	// System Info
	a.logger.Printf("ğŸ–¥ï¸  System: %s (%s %s)", m.SystemInfo.Hostname, m.SystemInfo.OS, m.SystemInfo.Platform)
	a.logger.Printf("   Uptime: %s", formatDuration(time.Duration(m.SystemInfo.Uptime)*time.Second))

	// CPU
	a.logger.Printf("ğŸ’» CPU Usage: %.2f%%", m.CPU.UsagePercent)
	a.logger.Printf("   Load Avg: %.2f (1m) | %.2f (5m) | %.2f (15m)",
		m.CPU.LoadAvg1, m.CPU.LoadAvg5, m.CPU.LoadAvg15)

	// Memory
	a.logger.Printf("ğŸ§  Memory: %.2f%% used (%s / %s)",
		m.Memory.UsedPercent,
		formatBytes(m.Memory.Used),
		formatBytes(m.Memory.Total))
	if m.Memory.SwapTotal > 0 {
		a.logger.Printf("   Swap: %.2f%% used (%s / %s)",
			m.Memory.SwapPercent,
			formatBytes(m.Memory.SwapUsed),
			formatBytes(m.Memory.SwapTotal))
	}

	// Disk
	a.logger.Println("ğŸ’¾ Disk Usage:")
	for _, disk := range m.Disk {
		a.logger.Printf("   %s: %.2f%% used (%s / %s)",
			disk.MountPoint,
			disk.UsedPercent,
			formatBytes(disk.Used),
			formatBytes(disk.Total))
	}

	// Network
	a.logger.Printf("ğŸŒ Network: â†‘ %s sent | â†“ %s received",
		formatBytes(m.Network.BytesSent),
		formatBytes(m.Network.BytesRecv))

	// Docker containers
	if len(m.Containers) > 0 {
		a.logger.Printf("ğŸ³ Containers: %d monitored", len(m.Containers))
		for _, container := range m.Containers {
			statusIcon := getContainerStatusIcon(container.State, container.Health)
			if container.State == "running" {
				a.logger.Printf("   %s %s: CPU %.1f%% | Mem %s (%.1f%%) | Restarts: %d",
					statusIcon,
					container.Name,
					container.CPUPercent,
					formatBytes(container.MemoryUsage),
					container.MemoryPercent,
					container.RestartCount)
			} else {
				a.logger.Printf("   %s %s: %s (exit code: %d)",
					statusIcon,
					container.Name,
					container.State,
					container.ExitCode)
			}
		}
	}

	// Output JSON for debugging
	if a.config.Agent.Name != "" {
		jsonData, _ := json.MarshalIndent(m, "", "  ")
		a.logger.Printf("\nğŸ“„ JSON Output:\n%s\n", string(jsonData))
	}
}

func getContainerStatusIcon(state, health string) string {
	if state == "running" {
		if health == "healthy" {
			return "ğŸŸ¢"
		} else if health == "unhealthy" {
			return "ğŸ”´"
		} else if health == "starting" {
			return "ğŸŸ¡"
		}
		return "ğŸŸ¢" // No health check defined
	} else if state == "exited" {
		return "âš«"
	} else if state == "restarting" {
		return "ğŸ”„"
	} else if state == "paused" {
		return "â¸ï¸"
	}
	return "âšª"
}

// Helper functions

func formatBytes(bytes uint64) string {
	const unit = 1024
	if bytes < unit {
		return fmt.Sprintf("%d B", bytes)
	}
	div, exp := uint64(unit), 0
	for n := bytes / unit; n >= unit; n /= unit {
		div *= unit
		exp++
	}
	return fmt.Sprintf("%.1f %ciB", float64(bytes)/float64(div), "KMGTPE"[exp])
}

func formatDuration(d time.Duration) string {
	days := d / (24 * time.Hour)
	d -= days * 24 * time.Hour
	hours := d / time.Hour
	d -= hours * time.Hour
	minutes := d / time.Minute

	if days > 0 {
		return fmt.Sprintf("%dd %dh %dm", days, hours, minutes)
	}
	if hours > 0 {
		return fmt.Sprintf("%dh %dm", hours, minutes)
	}
	return fmt.Sprintf("%dm", minutes)
}
